import { Project } from '@/types';

export const projects: Project[] = [
  {
    id: 'proj-1',
    title: 'Enterprise-Scale Real-Time Video Stream Analytics Pipeline',
    shortDescription: 'Fault-tolerant Kafka pipeline scaling to 8+ live video streams with sub-200ms inference latency',
    longDescription: 'Engineered a fault-tolerant Kafka producer with multithreading, systemd automation, and buffered pools, designed to sustain 24x7 ingestion of 100 events/sec per stream with ≤2% data loss. Orchestrated a modular consumer pipeline decoding timestamped frames, running TensorFlow Lite classification, and streaming results via Flask. Implemented millisecond-granular telemetry and integrated InfluxDB, Prometheus, Grafana, and Redpanda Console, cutting incident detection from ~5 minutes to ≤15 seconds.',
    category: 'Streaming',
    techStack: ['Apache Kafka', 'Docker', 'Flask', 'Grafana', 'Prometheus', 'InfluxDB', 'TensorFlow Lite'],
    metrics: '8+ concurrent streams, ≤2% data loss, sub-200ms latency',
    featured: true,
    date: 'June 2025',
    githubUrl: 'https://github.com/tejpreetsingh/video-stream-analytics',
  },
  {
    id: 'proj-2',
    title: 'Orchestrated ETL Pipeline with Airflow, Spark & Object Storage',
    shortDescription: 'Automated ETL workflows with Airflow DAG orchestration and PySpark transformations on MinIO',
    longDescription: 'Designed an Airflow DAG orchestrating raw data ingestion → Spark transformation → curated Parquet loads, automating 100% of ETL workflows. Deployed PySpark jobs reading/writing via S3A connector to MinIO buckets, processing ~15 GB datasets with 2x faster throughput than baseline. Integrated validation and schema checks as Airflow PythonOperators, catching 95% of data-quality anomalies pre-load.',
    category: 'ETL Pipelines',
    techStack: ['Apache Airflow', 'PySpark', 'MinIO', 'PostgreSQL', 'Docker'],
    metrics: '100% automated ETL, 70% less manual work, 95% anomaly catch rate',
    featured: false,
    date: 'February 2025',
    githubUrl: 'https://github.com/tejpreetsingh/etl-airflow-spark',
  },
  {
    id: 'proj-3',
    title: 'Data Lakehouse Prototype with Delta Lake on MinIO',
    shortDescription: 'Bronze-silver-gold architecture with ACID-compliant storage and time-travel queries',
    longDescription: 'Implemented a bronze-silver-gold architecture using Delta Lake on MinIO, delivering ACID-compliant storage for 20K+ records. Developed PySpark jobs with schema evolution and time-travel queries, cutting data reconciliation time by 50%. Demonstrated scalable batch transformations and compaction workflows, reducing query latency on curated tables by 40%.',
    category: 'Data Warehousing',
    techStack: ['PySpark', 'Delta Lake', 'MinIO', 'Jupyter'],
    metrics: '20K+ records, 50% faster reconciliation, 40% less query latency',
    featured: false,
    date: 'December 2024',
    githubUrl: 'https://github.com/tejpreetsingh/delta-lakehouse',
  },
  {
    id: 'proj-4',
    title: 'System Resource Monitoring Dashboard on macOS',
    shortDescription: 'Real-time system telemetry with automated ingestion into MariaDB and Grafana visualization',
    longDescription: 'Developed a Python service using psutil, capturing CPU, memory, and disk metrics every 10 seconds. Automated ingestion into MariaDB via launchd scheduling, ensuring uninterrupted long-term telemetry collection. Integrated Grafana with MariaDB, visualizing real-time trends and cutting manual monitoring effort by 30%.',
    category: 'Monitoring',
    techStack: ['Python', 'MariaDB', 'Grafana', 'Launchd', 'Docker'],
    metrics: '30% less manual monitoring, 10s metric granularity',
    featured: false,
    date: 'November 2024',
    githubUrl: 'https://github.com/tejpreetsingh/system-monitor',
  },
];
